{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66cbdd0-1d35-45a1-909d-96a81258bd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0234d6b-1a7a-4d7b-a325-e179b80cd832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32277b56-501c-4873-a46a-ee54f0d4db57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d973ec1-5517-4499-bebc-6065a2b57a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142123ea-6f78-41de-8d5e-db04f47dbcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083884b0-d02c-431b-9ff3-8a9261e29736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba02d4-82a6-4a16-af5d-4c5e46ceb39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b27b3-1339-4fe3-9e05-911488ed6e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48474402-2461-4968-80c5-7e3184d978f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aac4bf-efca-4cb7-aa92-5625c282f164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cbf8f46-6c19-4eed-95ef-d2255c7fad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Pillow\n",
      "Version: 9.2.0\n",
      "Summary: Python Imaging Library (Fork)\n",
      "Home-page: https://python-pillow.org\n",
      "Author: Alex Clark (PIL Fork Author)\n",
      "Author-email: aclark@python-pillow.org\n",
      "License: HPND\n",
      "Location: c:\\users\\yabakar\\anaconda3\\envs\\env_deeplearning\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: matplotlib\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc2bb4ba-bae2-442b-a961-f5cc0dc9a1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.8.2\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\yabakar\\anaconda3\\envs\\env_deeplearning\\lib\\site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ec422b-0f25-4312-85a7-ff42ea302f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: numpy\n",
      "Version: 1.23.1\n",
      "Summary: NumPy is the fundamental package for array computing with Python.\n",
      "Home-page: https://www.numpy.org\n",
      "Author: Travis E. Oliphant et al.\n",
      "Author-email: \n",
      "License: BSD\n",
      "Location: c:\\users\\yabakar\\anaconda3\\envs\\env_deeplearning\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: h5py, Keras-Preprocessing, matplotlib, mkl-fft, mkl-random, opt-einsum, scipy, tensorboard, tensorflow\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486bdab-2b9c-4416-98f7-9c5e8a5fbef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f00869-1feb-43b5-91d3-65ff8b210e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aeb414-2e47-416a-8dbb-a02698324358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782120f-240f-4c38-b2dc-f0d89b06fa30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90c694-0fb0-488b-b628-e20549f764aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3df38-51df-4cdf-94db-fc6bf38effe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfbc13-c6c5-4fd7-a98a-2040a95cf267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf78a4-3ab7-4bf0-933f-0b049b792a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee420064-c0d1-41fb-8a83-1d8bad17b0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f693e738-d33a-494e-9d25-ccdae4b4eafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conversion de la classe : 'daisy': 100%|██████████████████████████████████████████████| 20/20 [00:00<00:00, 120.82it/s]\n",
      "Conversion de la classe : 'dandelion': 100%|██████████████████████████████████████████| 20/20 [00:00<00:00, 141.16it/s]\n",
      "Conversion de la classe : 'rose': 100%|███████████████████████████████████████████████| 20/20 [00:00<00:00, 153.85it/s]\n",
      "Conversion de la classe : 'sunflower': 100%|██████████████████████████████████████████| 20/20 [00:00<00:00, 152.13it/s]\n",
      "Conversion de la classe : 'tulip': 100%|██████████████████████████████████████████████| 20/20 [00:00<00:00, 137.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def launchConversion(pathData, pathNumpy, resizeImg, imgSize):\n",
    "    \"\"\"\n",
    "    # Permet de lancer la conversion des images en tableau numpy\n",
    "    :param pathData: chemin ou sont les\n",
    "    :param pathNumpy:\n",
    "    :param resizeImg:\n",
    "    :param imgSize:\n",
    "    \"\"\"\n",
    "\n",
    "    #Pour chaque classe\n",
    "    for flowerClasse in os.listdir(pathData):\n",
    "        pathFlower = pathData + '\\\\' + flowerClasse\n",
    "        imgs = []\n",
    "\n",
    "        #Pour chaque image d'une classe, on la charge, resize et transforme en tableau\n",
    "        for imgFlower in tqdm(os.listdir(pathFlower), \"Conversion de la classe : '{}'\".format(flowerClasse)):\n",
    "            imgFlowerPath = pathFlower + '\\\\' + imgFlower\n",
    "            img = Image.open(imgFlowerPath)\n",
    "            img.load()\n",
    "            if resizeImg == True:\n",
    "                img = img.resize(size=imgSize)\n",
    "\n",
    "            data = np.asarray(img, dtype=np.float32)\n",
    "            imgs.append(data)\n",
    "\n",
    "        #Converti les gradients de pixels (allant de 0 à 255) vers des gradients compris entre 0 et 1\n",
    "        imgs = np.asarray(imgs) / 255.\n",
    "\n",
    "        #Enregistre une classe entiere en un fichier numpy\n",
    "        np.save(pathNumpy + '\\\\ ' + flowerClasse + '.npy', imgs)\n",
    "        \n",
    "        \n",
    "        \n",
    "pathNumpy = r'C:\\Users\\yabakar\\Documents\\Config\\diss1ia\\Projects\\DjangoRestAPI\\APIProjectFolder\\ImageClassifier\\numpy'\n",
    "pathData = r'C:\\Users\\yabakar\\Documents\\Config\\diss1ia\\Projects\\DjangoRestAPI\\APIProjectFolder\\ImageClassifier\\dataset'\n",
    "resizeImg = True\n",
    "imgSize = (50, 50)\n",
    "launchConversion(pathData, pathNumpy, resizeImg, imgSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d91ef5-4a17-401e-b708-d7b5ccc1af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from keras.optimizers import *\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c85cd3-fef9-4b49-9cac-d6dfef70bd56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134aab1-d905-480a-b28c-eebb343e81fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4722365-a024-4da8-8797-e41c120f49a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8bc3a-dc93-45e7-a1c9-f5dfbf8d7531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64db47-7ad9-44b5-8dea-2f3d8c803a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d56c06a-245e-4f45-a2a2-f68cec30da11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf1227-84be-4374-9376-9a14ebecf158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7c6be-32fe-4115-9469-c1d78a651f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdae4de-b014-4b51-b934-39b095bf32e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b03a4-416b-4f6c-ae68-682dc204d997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cf16-7088-4ffc-aa50-bf81daecb9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c2e118-b1a4-4e38-925c-202d70e7391c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a32246-c475-4593-baec-dd0cb7a25f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64c22f-bcf4-49bf-91dd-40a04db2bbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c4e17-f019-42b2-a589-f7657a83e1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba298915-0502-413b-8490-e77599d79d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617a9ab-a15a-4a7b-a067-cb20d7c1e8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c32396-5fc7-4df0-a666-b4f875899196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb1b91-e2a0-4f53-b572-838aaadd48a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea3f0b-4143-4bd1-afbc-7147cbeef198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad250a4c-f10a-4765-a2ef-15967537bb0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conversion de la classe : 'daisy': 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 142.92it/s]\n",
      "Conversion de la classe : 'dandelion': 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 125.02it/s]\n",
      "Conversion de la classe : 'rose': 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 125.04it/s]\n",
      "Conversion de la classe : 'sunflower': 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 124.96it/s]\n",
      "Conversion de la classe : 'tulip': 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 200.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# IMPORT\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def launchConversion(pathData, pathNumpy, resizeImg, imgSize):\n",
    "    \"\"\"\n",
    "    # Permet de lancer la conversion des images en tableau numpy\n",
    "    :param pathData: chemin ou sont les\n",
    "    :param pathNumpy:\n",
    "    :param resizeImg:\n",
    "    :param imgSize:\n",
    "    \"\"\"\n",
    "\n",
    "    #Pour chaque classe\n",
    "    for flowerClasse in os.listdir(pathData):\n",
    "        pathFlower = pathData + '\\\\' + flowerClasse\n",
    "        imgs = []\n",
    "\n",
    "        #Pour chaque image d'une classe, on la charge, resize et transforme en tableau\n",
    "        for imgFlower in tqdm(os.listdir(pathFlower), \"Conversion de la classe : '{}'\".format(flowerClasse)):\n",
    "            imgFlowerPath = pathFlower + '\\\\' + imgFlower\n",
    "            img = Image.open(imgFlowerPath)\n",
    "            img.load()\n",
    "            if resizeImg == True:\n",
    "                img = img.resize(size=imgSize)\n",
    "\n",
    "            data = np.asarray(img, dtype=np.float32)\n",
    "            imgs.append(data)\n",
    "\n",
    "        #Converti les gradients de pixels (allant de 0 à 255) vers des gradients compris entre 0 et 1\n",
    "        imgs = np.asarray(imgs) / 255.\n",
    "\n",
    "        #Enregistre une classe entiere en un fichier numpy\n",
    "        np.save(pathNumpy + '\\\\ ' + flowerClasse + '.npy', imgs)\n",
    "        \n",
    "        \n",
    "        \n",
    "pathNumpy = '.\\\\numpy'\n",
    "pathData = '.\\\\dataset'\n",
    "resizeImg = True\n",
    "imgSize = (50, 50)\n",
    "launchConversion(pathData, pathNumpy, resizeImg, imgSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f596e-4443-4730-b47e-72c4a01aa9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247f2a9-deaa-4ee0-b4da-4d6cc4a18f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88328943-9b04-40b5-83ed-026cae0c1a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION X TRAIN (4, 50, 50, 3)\n",
      "DIMENSION X TEST (1, 50, 50, 3)\n",
      "DIMENSION Y TRAIN (4, 5)\n",
      "DIMENSION Y TEST (1, 2)\n",
      "yala\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 5) and (None, 6) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 113>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m#On lance l'entrainement du modele\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myala\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m trainning \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 5) and (None, 6) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# IMPORT\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from keras.optimizers import *\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Classe permettant d'entrainer un modèle sur une jeu de données\n",
    "\"\"\"\n",
    "\n",
    "def get_labels(path):\n",
    "    \"\"\"\n",
    "    # Permet de recuperer les labels de nos classe, leurs indices dans le tableau et leur matrix binaire one hot encoder\n",
    "    :param path: chemin ou sont stocké nos fichiers Numpy\n",
    "    \"\"\"\n",
    "\n",
    "    labels = [file.replace('.npy', '') for file in os.listdir(path) if file.endswith('.npy')]\n",
    "    label_indices = np.arange(0, len(labels))\n",
    "    return labels, label_indices, to_categorical(label_indices)\n",
    "\n",
    "\n",
    "\n",
    "def get_train_test(train_ratio, pathData):\n",
    "    \"\"\"\n",
    "    # Retourner le dataset melanger en dataset d'entrainement et de validation selon un ratio\n",
    "    :param train_ratio: permet de gerer la part entre dataset de train et de validation\n",
    "    :param pathData: chemin des fichiers numpy\n",
    "    \"\"\"\n",
    "\n",
    "    labels, _, _ = get_labels(pathData)\n",
    "    classNumber = 0\n",
    "\n",
    "    #On init avec le premier tableau pour avoir les bonnes dimensions pour la suite\n",
    "    X = data = np.load(pathData + '\\\\' + labels[0] + '.npy')\n",
    "    Y = np.zeros(X.shape[0])\n",
    "    dimension = X[0].shape\n",
    "    classNumber += 1\n",
    "\n",
    "\n",
    "    #On ajoute le reste des fichiers numpy de nos classes\n",
    "    for i, label in enumerate(labels[1:]):\n",
    "        data = np.load(pathData + '\\\\' + label + '.npy')\n",
    "        X = np.vstack((X, data))\n",
    "        Y = np.append(Y, np.full(data.shape[0], fill_value=(i+1)))\n",
    "        classNumber += 1\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=train_ratio)\n",
    "    return X_train, X_test, to_categorical(Y_train), to_categorical(Y_test), classNumber, dimension\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Fonction main\n",
    "\"\"\"\n",
    "\n",
    "#Definition des chemins et autres variables\n",
    "pathData = '.\\\\numpy'\n",
    "trainRatio = 0.8\n",
    "epochs = 1000\n",
    "batch_size = 16\n",
    "earlyStopPatience = 5\n",
    "\n",
    "#Definition des callbacks\n",
    "\n",
    "#Permet de retourner 4 metrics de suivi a chaque iteration\n",
    "csv_logger = CSVLogger('.\\\\logs\\\\log_moModel.csv', append=True, separator=',')\n",
    "\n",
    "#Permet de stopper l'entrainement quand le modèle n'entraine pluss\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=earlyStopPatience, verbose=0, mode='auto')\n",
    "\n",
    "#Permet de sauvegarder le model a chaque iteration si il est meilleur que le precedent\n",
    "check = ModelCheckpoint('.\\\\trainedModel\\\\moModel.hdf5', monitor='val_loss', verbose=0,\n",
    "                            save_best_only=True, save_weights_only=False, mode='auto')\n",
    "\n",
    "#Recuperation de nos data pré traité\n",
    "x_train, x_test, y_train, y_test, classNumber, dimension = get_train_test(trainRatio, pathData)\n",
    "\n",
    "#On verifie les dimensions de nos données\n",
    "print('DIMENSION X TRAIN ' + str(x_train.shape))\n",
    "print('DIMENSION X TEST ' + str(x_test.shape))\n",
    "print('DIMENSION Y TRAIN ' + str(y_train.shape))\n",
    "print('DIMENSION Y TEST ' + str(y_test.shape))\n",
    "\n",
    "#On creer le modele\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(dimension[0], dimension[1], dimension[2])))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(classNumber, activation='softmax'))\n",
    "\n",
    "#On compile le modele\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.0),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#On lance l'entrainement du modele\n",
    "print(\"yala\")\n",
    "trainning = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks=[early, check,csv_logger])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185eaa33-d994-4834-884b-19956e18c564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9a38d2eb-e76b-4e4d-924d-05784d38f3cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m pathLogs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mlog_moModel.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     56\u001b[0m pathSaveGraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 57\u001b[0m \u001b[43mdisplayGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathLogs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpathSaveGraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36mdisplayGraph\u001b[1;34m(pathLog, pathSaveGraph)\u001b[0m\n\u001b[0;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(pathLog)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# split into input (X) and output (Y) variables\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m plot(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAIN_VAL_Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupper left\u001b[39m\u001b[38;5;124m'\u001b[39m,pathSaveGraph)\n\u001b[0;32m     21\u001b[0m plot(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAIN_VAL_Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupper left\u001b[39m\u001b[38;5;124m'\u001b[39m,pathSaveGraph)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "# IMPORT\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "# Classe permettant de génerer 4 graphiques de suivit de métriques durant l'entrainement d'un modèle\n",
    "# Train accuracy, Train loss, Validation accuracy, Validation loss\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def displayGraph(pathLog,pathSaveGraph):\n",
    "    \"\"\"\n",
    "    # Fonction permettant de creer nos graph de suivi de metriques\n",
    "    :param pathLog: chemin du CSV contenant nos metrics\n",
    "    :param pathSaveGraph: chemin de destination pour sauvegarder nos 4 graphiques en jpg\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(pathLog)\n",
    "    # split into input (X) and output (Y) variables\n",
    "    plot(data['epoch'], data['acc'], data['val_acc'], 'TRAIN_VAL_Accuracy', 'Epoch', 'Accuracy', 'upper left',pathSaveGraph)\n",
    "    plot(data['epoch'], data['loss'], data['val_loss'], 'TRAIN_VAL_Loss', 'Epoch', 'Loss', 'upper left',pathSaveGraph)\n",
    "\n",
    "    \n",
    "    \n",
    "def plot(X, Y, Y2, title, xLabel, yLabel, legendLoc, pathSaveGraph):\n",
    "    \"\"\"\n",
    "    # Fonction d'affichage de graph\n",
    "    :param X: correspond au nombre d'époch\n",
    "    :param Y: correspond a la courbe accuracy\n",
    "    :param Y2: correspond a la courbe loss\n",
    "    :param title: titre du graphique\n",
    "    :param xLabel: label des abcisses\n",
    "    :param yLabel: label des ordonnees\n",
    "    :param legendLoc: legende\n",
    "    :param pathSaveGraph: chemin de sauvegarde pour les graphiques\n",
    "    \"\"\"\n",
    "\n",
    "   #On trace nos differentes courbes\n",
    "    plt.plot(Y)\n",
    "    plt.plot(Y2)\n",
    "   #titre du graph, legende...\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xLabel)\n",
    "    plt.ylabel(yLabel)\n",
    "    plt.legend(['train', 'val'], loc=legendLoc)\n",
    "   #Pour avoir un courbe propre qui demarre à 0\n",
    "    plt.xlim(xmin=0.0, xmax=max(X))\n",
    "    plt.savefig(pathSaveGraph +'\\\\' + title)\n",
    "    plt.figure()\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "#Definition des chemins d'acces a notre fichier log\n",
    "pathLogs = '.\\\\logs\\\\log_moModel.csv'\n",
    "pathSaveGraph = '.\\\\graph'\n",
    "displayGraph(pathLogs,pathSaveGraph)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "070d0fe3-d651-4039-bbbd-4fe0f5dd7793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prediction de la classe 'daisy': 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 11.72it/s]\n",
      "Prediction de la classe 'dandelion': 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 11.91it/s]\n",
      "Prediction de la classe 'rose': 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.93it/s]\n",
      "Prediction de la classe 'sunflower': 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 10.38it/s]\n",
      "Prediction de la classe 'tulip': 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision : 40.000%\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IMPORT\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Classe permettant de génerer une matrice de confusion à partir d'un dataset de test et d'un modèle entrainé\n",
    "# au préalable\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generateMatrix(model, datasetTestPath, imageSize, destinationMatrix):\n",
    "\t\"\"\"\n",
    "\t# Fonction qui va construire notre matrice de confusion\n",
    "\t:param model: chemin du modèle à charger pour realiser la prediction\n",
    "\t:param datasetTestPath: chemin du dataset contenant nos images de test\n",
    "\t:param imageSize: definit la taille de l'ensemble de nos images\n",
    "\t:param destinationMatrix: définit le chemin ou va être sauvegardé notre matrice sous format d'image\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\n",
    "\t#Les tableaux contenanrt les predictions\n",
    "\ty_true = []\n",
    "\ty_pred = []\n",
    "\n",
    "\ttotal = 0\n",
    "\tsuccess = 0\n",
    "\tindex = 0\n",
    "\n",
    "\tprint('\\nEvaluation :')\n",
    "\t#On parcours notre dataset de test\n",
    "\tfor root, dirs, files in os.walk(datasetTestPath):\n",
    "\t\tfor mydir in dirs:\n",
    "\t\t\tfor sample in tqdm(os.listdir(root + '\\\\' + mydir), \"Prediction de la classe '{}'\".format(mydir)):\n",
    "\n",
    "\t\t\t\tsample_path = root + '\\\\' + mydir + '\\\\' + sample\n",
    "\t\t\t\t#Chargement et traitement de l'image\n",
    "\t\t\t\timg = Image.open(sample_path)\n",
    "\t\t\t\timg.load()\n",
    "\t\t\t\timg = img.resize(size=imageSize)\n",
    "\t\t\t\timg = np.asarray(img) / 255.\n",
    "\t\t\t\t#On reshape pour etre de la forme (nbImage,hauteurImage,largeurImage,nbCanaux)\n",
    "\t\t\t\timg = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n",
    "\t\t\t\t#Prediction de notre modele\n",
    "\t\t\t\tpred = np.argmax(model.predict(img))\n",
    "\n",
    "\t\t\t\ttotal += 1\n",
    "\t\t\t\tif pred == index:\n",
    "\t\t\t\t\tsuccess += 1\n",
    "\n",
    "\t\t\t\ty_true.append(index)\n",
    "\t\t\t\ty_pred.append(pred)\n",
    "\n",
    "\t\t\tindex += 1\n",
    "\n",
    "\t#Precision de notre modele sur notre jeu de test en entier\n",
    "\taccuracy = (success / total) * 100.\n",
    "\tprint('\\nPrecision : {0:.3f}%'.format(accuracy))\n",
    "\n",
    "\n",
    "\tcnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\tnp.set_printoptions(precision=2)\n",
    "\n",
    "\t# Plot normalized confusion matrix\n",
    "\tplt.figure()\n",
    "\n",
    "\tcmap = plt.cm.Blues\n",
    "\tclasses = ['marguerite', 'pissenlit', 'rose', 'tournesol', 'tulipe']\n",
    "\ttitle = 'Confusion matrix'\n",
    "\n",
    "\tcnf_matrix = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\t#Legende de notre matrice\n",
    "\tplt.imshow(cnf_matrix, interpolation='nearest', cmap=cmap)\n",
    "\tplt.title(title)\n",
    "\tplt.colorbar()\n",
    "\ttick_marks = np.arange(len(classes))\n",
    "\tplt.xticks(tick_marks, classes, rotation=45)\n",
    "\tplt.yticks(tick_marks, classes)\n",
    "\n",
    "\tfmt = '.2f'\n",
    "\tthresh = cnf_matrix.max() / 2.\n",
    "\tfor i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "\t\tplt.text(j, i, format(cnf_matrix[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "\tplt.ylabel('True label')\n",
    "\tplt.xlabel('Predicted label')\n",
    "\tplt.tight_layout()\n",
    "\n",
    "\t#On sauvegarde notre matrice en image\n",
    "\tplt.savefig(destinationMatrix + '\\\\' + 'confusionMatrix')\n",
    "\n",
    "\n",
    "    \n",
    "#On definit les chemins de nos divers ressources\n",
    "modelPath = '.\\\\trainedModel\\\\moModel.hdf5'\n",
    "datasetTestPath = '.\\\\datasetTest'\n",
    "destinationMatrix = '.\\\\graph'\n",
    "imageSize = (50, 50)\n",
    "model = load_model(modelPath)\n",
    "\n",
    "generateMatrix(model, datasetTestPath, imageSize, destinationMatrix)\n",
    "\n",
    "\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f45630-e7eb-4773-8dbe-cd5bcf57bac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28980e7a-2b82-4af9-9fcf-e42035e014e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff8be3c6-049e-42b8-8495-918e83f964f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle :\n",
      "\n",
      "\n",
      "Model chargé.\n",
      "\n",
      "----------\n",
      " Prediction :\n",
      "     marguerite : 0.70%\n",
      "     pissenlit : 2.30%\n",
      "     rose : 26.77%\n",
      "     tournesol : 20.67%\n",
      "     tulipe : 49.56%\n",
      "\n",
      "RESULTAT : tulipe : 49.56%\n",
      "temps prediction : 0.23secs\n",
      "----------\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "#IMPORT\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def predict(modelPath,imagePath, imageSize, label):\n",
    "    \"\"\"\n",
    "    # Fonction qui permet de convertir une image en array, de charger le modele et de lui injecter notre image pour une prediction\n",
    "    :param modelPath: chemin du modèle au format hdf5\n",
    "    :param imagePath: chemin de l'image pour realiser une prediction\n",
    "    :param imageSize: défini la taille de l'image. IMPORTANT : doit être de la même taille que celle des images\n",
    "    du dataset d'entrainements\n",
    "    :param label: nom de nos 5 classes de sortie\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Chargement du modele\n",
    "    print(\"Chargement du modèle :\\n\")\n",
    "    model = load_model(modelPath)\n",
    "    print(\"\\nModel chargé.\")\n",
    "\n",
    "    #Chargement de notre image et traitement\n",
    "    data = []\n",
    "    img = Image.open(imagePath)\n",
    "    img.load()\n",
    "    img = img.resize(size=imageSize)\n",
    "    img = np.asarray(img) / 255.\n",
    "    data.append(img)\n",
    "    data = np.asarray(data)\n",
    "\n",
    "    #On reshape pour correspondre aux dimensions de notre modele\n",
    "    # Arg1 : correspond au nombre d'image que on injecte\n",
    "    # Arg2 : correspond a la largeur de l'image\n",
    "    # Arg3 : correspond a la hauteur de l'image\n",
    "    # Arg4 : correspond au nombre de canaux de l'image (1 grayscale, 3 couleurs)\n",
    "    dimension = data[0].shape\n",
    "\n",
    "    #Reshape pour passer de 3 à 4 dimension pour notre réseau\n",
    "    data = data.astype(np.float32).reshape(data.shape[0], dimension[0], dimension[1], dimension[2])\n",
    "\n",
    "    #On realise une prediction\n",
    "    prediction = model.predict(data)\n",
    "\n",
    "\n",
    "    #On recupere le numero de label qui a la plus haut prediction\n",
    "    maxPredict = np.argmax(prediction)\n",
    "\n",
    "    #On recupere le mot correspondant à l'indice precedent\n",
    "    word = label[maxPredict]\n",
    "    pred = prediction[0][maxPredict] * 100.\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    #On affiche les prédictions\n",
    "    print()\n",
    "    print('----------')\n",
    "    print(\" Prediction :\")\n",
    "    for i in range(0, len(label)):\n",
    "        print('     ' + label[i] + ' : ' + \"{0:.2f}%\".format(prediction[0][i] * 100.))\n",
    "\n",
    "    print()\n",
    "    print('RESULTAT : ' + word + ' : ' + \"{0:.2f}%\".format(pred))\n",
    "    print('temps prediction : ' + \"{0:.2f}secs\".format(end-start))\n",
    "\n",
    "    print('----------')\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "# On definit les chemins d'acces au différentes hyper parametre\n",
    "\"\"\"\n",
    "\n",
    "modelPath = '.\\\\trainedModel\\\\moModel.hdf5'\n",
    "imagePath =  '.\\\\testImage\\\\rose.jpg'\n",
    "imageSize = (50,50)\n",
    "label = ['marguerite', 'pissenlit', 'rose', 'tournesol', 'tulipe']\n",
    "\n",
    "predict(modelPath, imagePath,imageSize, label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774fe30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "483355bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "threshold must be non-NAN, try sys.maxsize for untruncated representation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_printoptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m FRmodel \u001b[38;5;241m=\u001b[39m faceRecoModel(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m96\u001b[39m))\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, FRmodel\u001b[38;5;241m.\u001b[39mcount_params())\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\numpy\\core\\arrayprint.py:277\u001b[0m, in \u001b[0;36mset_printoptions\u001b[1;34m(precision, threshold, edgeitems, linewidth, suppress, nanstr, infstr, formatter, sign, floatmode, legacy)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_printoptions\u001b[39m(precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, edgeitems\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    118\u001b[0m                      linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, suppress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, nanstr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, infstr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    119\u001b[0m                      formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sign\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, floatmode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, legacy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    Set printing options.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     opt \u001b[38;5;241m=\u001b[39m \u001b[43m_make_options_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medgeitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m                             \u001b[49m\u001b[43msuppress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanstr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfstr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mfloatmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# formatter is always reset\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformatter\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m formatter\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\numpy\\core\\arrayprint.py:103\u001b[0m, in \u001b[0;36m_make_options_dict\u001b[1;34m(precision, threshold, edgeitems, linewidth, suppress, nanstr, infstr, sign, formatter, floatmode, legacy)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold must be numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(threshold):\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold must be non-NAN, try \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msys.maxsize for untruncated representation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# forbid the bad precision arg as suggested by issue #18254\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: threshold must be non-NAN, try sys.maxsize for untruncated representation"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# from fr_utils import *\n",
    "# from inception_blocks_v2 import *\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n",
    "print(\"Total Params:\", FRmodel.count_params())\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d17785",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 182] Le système d’exploitation ne peut pas exécuter %1. Error loading \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m join\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_url, check_integrity, list_dir, list_files\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOmniglot\u001b[39;00m(data\u001b[38;5;241m.\u001b[39mDataset):\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:672\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:632\u001b[0m, in \u001b[0;36m_load_backward_compatible\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\wandb\\sdk\\lib\\import_hooks.py:195\u001b[0m, in \u001b[0;36m_ImportHookChainedLoader.load_module\u001b[1;34m(self, fullname)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, fullname: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 195\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     notify_module_loaded(module)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\torch\\__init__.py:129\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    127\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    128\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 182] Le système d’exploitation ne peut pas exécuter %1. Error loading \"C:\\Users\\yabakar\\Anaconda3\\envs\\env_deeplearning\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from os.path import join\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "from .utils import download_url, check_integrity, list_dir, list_files\n",
    "\n",
    "\n",
    "class Omniglot(data.Dataset):\n",
    "    \"\"\"`Omniglot <https://github.com/brendenlake/omniglot>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``omniglot-py`` exists.\n",
    "        background (bool, optional): If True, creates dataset from the \"background\" set, otherwise\n",
    "            creates from the \"evaluation\" set. This terminology is defined by the authors.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset zip files from the internet and\n",
    "            puts it in root directory. If the zip files are already downloaded, they are not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "    folder = 'omniglot-py'\n",
    "    download_url_prefix = 'https://github.com/brendenlake/omniglot/raw/master/python'\n",
    "    zips_md5 = {\n",
    "        'images_background': '68d2efa1b9178cc56df9314c21c6e718',\n",
    "        'images_evaluation': '6b91aef0f799c5bb55b94e3f2daec811'\n",
    "    }\n",
    "\n",
    "    def __init__(self, root, background=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        self.root = join(os.path.expanduser(root), self.folder)\n",
    "        self.background = background\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        self.target_folder = join(self.root, self._get_target_folder())\n",
    "        self._alphabets = list_dir(self.target_folder)\n",
    "        self._characters = sum([[join(a, c) for c in list_dir(join(self.target_folder, a))]\n",
    "                                for a in self._alphabets], [])\n",
    "        self._character_images = [[(image, idx) for image in list_files(join(self.target_folder, character), '.png')]\n",
    "                                  for idx, character in enumerate(self._characters)]\n",
    "        self._flat_character_images = sum(self._character_images, [])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._flat_character_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target character class.\n",
    "        \"\"\"\n",
    "        image_name, character_class = self._flat_character_images[index]\n",
    "        image_path = join(self.target_folder, self._characters[character_class], image_name)\n",
    "        image = Image.open(image_path, mode='r').convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            character_class = self.target_transform(character_class)\n",
    "\n",
    "        return image, character_class\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        zip_filename = self._get_target_folder()\n",
    "        if not check_integrity(join(self.root, zip_filename + '.zip'), self.zips_md5[zip_filename]):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def download(self):\n",
    "        import zipfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        filename = self._get_target_folder()\n",
    "        zip_filename = filename + '.zip'\n",
    "        url = self.download_url_prefix + '/' + zip_filename\n",
    "        download_url(url, self.root, zip_filename, self.zips_md5[filename])\n",
    "        print('Extracting downloaded file: ' + join(self.root, zip_filename))\n",
    "        with zipfile.ZipFile(join(self.root, zip_filename), 'r') as zip_file:\n",
    "            zip_file.extractall(self.root)\n",
    "\n",
    "    def _get_target_folder(self):\n",
    "        return 'images_background' if self.background else 'images_evaluation'\n",
    "\n",
    "\n",
    "def loadimgs(path,n = 0):\n",
    "    '''\n",
    "    path => Path of train directory or test directory\n",
    "    '''\n",
    "    X=[]\n",
    "    y = []\n",
    "    cat_dict = {}\n",
    "    lang_dict = {}\n",
    "    curr_y = n\n",
    "    \n",
    "    # we load every alphabet seperately so we can isolate them later\n",
    "    for alphabet in os.listdir(path):\n",
    "        print(\"loading alphabet: \" + alphabet)\n",
    "        lang_dict[alphabet] = [curr_y,None]\n",
    "        alphabet_path = os.path.join(path,alphabet)\n",
    "        \n",
    "        # every letter/category has it's own column in the array, so  load seperately\n",
    "        for letter in os.listdir(alphabet_path):\n",
    "            cat_dict[curr_y] = (alphabet, letter)\n",
    "            category_images=[]\n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            \n",
    "            # read all the images in the current category\n",
    "            for filename in os.listdir(letter_path):\n",
    "                image_path = os.path.join(letter_path, filename)\n",
    "                image = imread(image_path)\n",
    "                category_images.append(image)\n",
    "                y.append(curr_y)\n",
    "            try:\n",
    "                X.append(np.stack(category_images))\n",
    "            # edge case  - last one\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1\n",
    "    y = np.vstack(y)\n",
    "    X = np.stack(X)\n",
    "    return X,y,lang_dict\n",
    "\n",
    "def get_batch(batch_size,s=\"train\"):\n",
    "    \"\"\"\n",
    "    Create batch of n pairs, half same class, half different class\n",
    "    \"\"\"\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "        categories = train_classes\n",
    "    else:\n",
    "        X = Xval\n",
    "        categories = val_classes\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n",
    "    \n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "    pairs=[np.zeros((batch_size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    # initialize vector for the targets\n",
    "    targets=np.zeros((batch_size,))\n",
    "    \n",
    "    # make one half of it '1's, so 2nd half of batch has same class\n",
    "    targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = rng.randint(0, n_examples)\n",
    "        pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        idx_2 = rng.randint(0, n_examples)\n",
    "        \n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category  \n",
    "        else: \n",
    "            # add a random number to the category modulo n classes to ensure 2nd image has a different category\n",
    "            category_2 = (category + rng.randint(1,n_classes)) % n_classes\n",
    "        \n",
    "        pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "    \n",
    "return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65ad5a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " flatten_input (InputLayer)     [(None, 28, 28)]     0           []                               \n",
      "                                                                                                  \n",
      " flatten_1_input (InputLayer)   [(None, 28, 28)]     0           []                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 784)          0           ['flatten_input[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 784)          0           ['flatten_1_input[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          100480      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          100480      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256)          0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            257         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 201,217\n",
      "Trainable params: 201,217\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bew3x6mu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▂▂▃▄▅▆▆██</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▆▆▆▅▅▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>GFLOPs</td><td>0.0002</td></tr><tr><td>accuracy</td><td>0.52511</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.68926</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">daily-rain-3</strong>: <a href=\"https://wandb.ai/youssouf10/siamese/runs/bew3x6mu\" target=\"_blank\">https://wandb.ai/youssouf10/siamese/runs/bew3x6mu</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220825_122009-bew3x6mu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bew3x6mu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\yabakar\\Documents\\ImageClassifier\\wandb\\run-20220825_122414-15rh2khm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/youssouf10/siamese/runs/15rh2khm\" target=\"_blank\">different-wave-4</a></strong> to <a href=\"https://wandb.ai/youssouf10/siamese\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   1/7500 [..............................] - ETA: 59:44 - loss: 0.7100 - accuracy: 0.5625WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0018s). Check your callbacks.\n",
      "7500/7500 [==============================] - 25s 3ms/step - loss: 0.6945 - accuracy: 0.5030\n",
      "Epoch 2/10\n",
      "7500/7500 [==============================] - 22s 3ms/step - loss: 0.6934 - accuracy: 0.5039\n",
      "Epoch 3/10\n",
      "7500/7500 [==============================] - 22s 3ms/step - loss: 0.6931 - accuracy: 0.5065\n",
      "Epoch 4/10\n",
      "7500/7500 [==============================] - 20s 3ms/step - loss: 0.6927 - accuracy: 0.5076\n",
      "Epoch 5/10\n",
      "7500/7500 [==============================] - 22s 3ms/step - loss: 0.6923 - accuracy: 0.5122\n",
      "Epoch 6/10\n",
      "7500/7500 [==============================] - 21s 3ms/step - loss: 0.6917 - accuracy: 0.5143\n",
      "Epoch 7/10\n",
      "7500/7500 [==============================] - 23s 3ms/step - loss: 0.6910 - accuracy: 0.5157\n",
      "Epoch 8/10\n",
      "7500/7500 [==============================] - 22s 3ms/step - loss: 0.6905 - accuracy: 0.5144\n",
      "Epoch 9/10\n",
      "7500/7500 [==============================] - 21s 3ms/step - loss: 0.6899 - accuracy: 0.5174\n",
      "Epoch 10/10\n",
      "7500/7500 [==============================] - 25s 3ms/step - loss: 0.6890 - accuracy: 0.5205\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Concatenate, Dot, Lambda, Input\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "       # load data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       # make pairs\n",
    "def make_pairs(x, y):\n",
    "    num_classes = max(y) + 1\n",
    "    digit_indices = [np.where(y == i)[0] for i in range(num_classes)]\n",
    "\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    for idx1 in range(len(x)):\n",
    "        # add a matching example\n",
    "        x1 = x[idx1]\n",
    "        label1 = y[idx1]\n",
    "        idx2 = random.choice(digit_indices[label1])\n",
    "        x2 = x[idx2]\n",
    "        \n",
    "        pairs += [[x1, x2]]\n",
    "        labels += [1]\n",
    "    \n",
    "        # add a not matching example\n",
    "        label2 = random.randint(0, num_classes-1)\n",
    "        while label2 == label1:\n",
    "            label2 = random.randint(0, num_classes-1)\n",
    "\n",
    "        idx2 = random.choice(digit_indices[label2])\n",
    "        x2 = x[idx2]\n",
    "        \n",
    "        pairs += [[x1, x2]]\n",
    "        labels += [0]\n",
    "\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "pairs_train, labels_train = make_pairs(x_train, y_train)\n",
    "pairs_test, labels_test = make_pairs(x_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       # take a peek at the data\n",
    "# plt.imshow(pairs_train[400,1])\n",
    "# print(labels_train[4])\n",
    "\n",
    "\n",
    "\n",
    "       # Weights not shared\n",
    "\n",
    "seq1 = Sequential()\n",
    "seq1.add(Flatten(input_shape=(28,28)))\n",
    "seq1.add(Dense(128, activation='relu'))\n",
    "\n",
    "seq2 = Sequential()\n",
    "seq2.add(Flatten(input_shape=(28,28)))\n",
    "seq2.add(Dense(128, activation='relu'))\n",
    "\n",
    "merge_layer = Concatenate()([seq1.output, seq2.output])\n",
    "dense_layer = Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "model = Model(inputs=[seq1.input, seq2.input], outputs=dense_layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(project=\"siamese\")\n",
    "model.fit([pairs_train[:,0], pairs_train[:,1]], labels_train[:], batch_size=16, epochs= 10, callbacks=[WandbCallback()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46429991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34545de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f0d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
